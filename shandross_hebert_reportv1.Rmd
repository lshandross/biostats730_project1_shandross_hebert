---
title: "Survival Prediction After Heart Failure"
author: "Li Shandross and Scott Hebert"
date: '2022-12-02'
output:
   pdf_document: 
      toc: true
      toc_depth: 2
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
#knitr::opts_chunk$set(eval = FALSE)
knitr::opts_chunk$set(message = FALSE)
knitr::opts_chunk$set(warning = FALSE)
```

```{r, include = FALSE}
library(tidyverse)
library(bayesplot)
library(tidybayes)
library(rstanarm)
library(brms)
library(arm)
library(patchwork)
library(ROCR)
library(pander)
```

```{r, include = FALSE}
# Reading in data
set.seed(1234)
heart <- read.csv("heart_failure_clinical_records_dataset.csv")
heartt <- heart[sample(nrow(heart), 100), ]
head(heartt)
```

# Abstract

[maximum 200 words]

# Introduction

Cardiovascular diseases (CVDs) are a group of disorders of the heart and blood vessels which account for roughly 17 million deaths worldwide annually. CVDs are especially prevalent in industrialized countries, yet current evaluation of the disease progression in various CVDs, especially heart failure, remains lacking. Heart failure is one type of CVD that occurs when the heart fails to pump sufficient blood to the rest of the body. Prediction of heart failure outcome is of vital importance in clinical practice throughout the world but has not yielded promising results.  

A study by Chicco & Jurman highlighted the potential of machine learning methods to provide physicians with better tools to predict heart failure patient outcomes. The authors analyzed a data set of 299 heart failure patient medical records originally released by Ahmad and colleagues. Chicco & Jurman investigated ten types of machine learning methods using all predictors in the dataset and performed feature selection to determine the most important predictors. Random forests yielded the best results of the ten techniques while feature selection showed serenade creatinine and ejection fraction to be the best predictors. A random forests model using only these top two predictors outperformed models with all available predictors, which also included age, anaemia, high blood pressure, blood creatinine phosphokinase, diabetes, blood platelets, sex, serum sodium, and smoking status.  

This project utilizes the same dataset as Chicco & Jurman, but it utilizes a Bayesian logistic regression model in order to compare Bayesian methods to the machine learning methods of the reference paper.  


# Methods 
## Choice of models and model equations
We chose to compare four total models to better examine which aspects of the Bayesian models contribute to model accuracy. 
  
First, a Bayesian logistic regression model with all predictors was formulated: 

$y_i \sim Bern(\theta_i)$   
$logit(\theta_i) = \beta_0 + \beta_1 a_i + \beta_2 m_i + \beta_3 h_i + \beta_4 k_i + \beta_5 d_i + \beta_6 e_i + \beta_7 p_i + \beta_8 x_i + \beta_9 c_i + \beta_{10} s_i + \beta_{11} g_i$  

Then, a model with only the two predictors mentioned by the reference paper to be most important (known henceforth as the "reduced model") was created: 

$y_i \sim Bern(\theta_i)$   
$logit(\theta_i) = \beta_0 + \beta_1 e_i + \beta_2 c_i$ 

An intercept-only model was created for reference: 

$y_i \sim Bern(\theta_i)$   
$logit(\theta_i) = \beta_0$ 

Lastly, a model with horseshoe priors was formulated as a method of variable selection: 

$y_i \sim Bern(\theta_i)$   
$logit(\theta_i) = \beta_0 + \beta_1 a_i + \beta_2 m_i + \beta_3 h_i + \beta_4 k_i + \beta_5 d_i + \beta_6 e_i + \beta_7 p_i + \beta_8 x_i + \beta_9 c_i + \beta_{10} s_i + \beta_{11} g_i$   
$\beta_0 \sim N(0, 1)$   
$\beta_j | \lambda_j, \tau \sim N(0, \lambda_j \tau)$   
$\lambda_j \sim C^+(0, 1)$, $j=1, \cdots, P$   
$\tau \sim C^+(0, \tau_0)$ where $\tau_0 = \frac{p_0}{P-p_0} \frac{\sigma}{\sqrt{n}}$   
$\sigma$ is approximated with pseudo variance $\tilde{\sigma}^2=1/\mu(1-\mu)$ for a non-gaussian link  

(Horseshoe priors are described further in the Horseshoe priors subsection.)  

where:

ai refers to patient age in years,

mi refers to the presence of anaemia, 

hi refers to the presence of high blood pressure,

ki refers to blood creatinine phosphokinase level in mcg/L,

di refers to the presence of diabetes,

ei refers to ejection fraction (percentage of blood leaving the heart upon each contraction),

pi refers to blood platelets in kiloplatelets/mL,

xi refers to sex (M/F),

ci refers to serum creatinine in mg/dL,

si refers to serum sodium in mEq/L,

gi refers to whether the patient smokes

## Model implementation
Models are implemented using the packages `brms` and `rstanarm` with additional model checks performed using `arm`, `tidybayes`, and `bayesplot`. The horseshoe prior model is fit and checked using functions from `rstanarm` (unlike the other three models) because `brms` only supports the use of horseshoe priors for linear regression, not logistic regression.   
   
## Horseshoe priors
The horseshoe is a type of Bayesian prior (developed by Piironen and Vehtari) that serves as a shrinkage method to improve model fit. This prior is named for its U-shape that resembles a horseshoe and determines the constraints on coefficient estimates. Coefficients associated with predictors weakly supported by the data are shrunk very close to zero while coefficients more strongly supported by the data experience minimal shrinkage.  

We chose to explore usage of the horseshoe prior for several reasons. First, results from the reference paper showed that the models with ejection fraction and serum creatinine as the only predictors outperformed models using all predictors. This suggests that constraining coefficient estimates of unimportant predictors may yield better prediction accuracy. Second, as the horseshoe prior only shrinks coefficients of unsupported variables towards zero, it provides an interesting compromise between the reduced model and the full model described above. Third, the horseshoe prior was out of the scope of the Applied Bayesian Modeling class, and this project serves as an opportunity to learn how to apply a new type of prior.  

## Initial checks and validation
Before running the models on the full dataset, we first performed an initial test of each model on a smaller sample of the data with few iterations for some preliminary validation. The models passed these initial checks, allowing us to proceed with the chosen four models.   

We began fitting the models using 1000 iterations with 500 as warm up, spread across four chains. However, this created warnings of divergence and low bulk ESS for the horseshoe prior model, though other MCMC diagnostics like Rhat values of 1.0 and n_eff values greater than 225 indicated increasing warm-up and the number of iterations would be sufficient to fix the problem. We chose to manually increase certain defaults for all of the models for consistency; for example, we used 1,000 warmup iterations and set maximum tree depth to 20 (increased from the default value of 10).  

```{r, include = F}
# Full model
fullmod <- brm(formula = DEATH_EVENT ~ ., 
                data = heart,
                family = bernoulli(link = "logit"),
                chains = 4, 
                iter = 2000,
                warmup = 1000,
               control = list(max_treedepth = 20),
                cores = getOption("mc.cores", 12), 
                file = "output/fullmod4")
# summary(fullmod)

# Reduced model
redmod <- brm(formula = DEATH_EVENT ~ ejection_fraction + serum_creatinine, 
                data = heart,
                family = bernoulli(link = "logit"),
                chains = 4, 
                iter = 2000,
                warmup = 1000,
               control = list(max_treedepth = 20),
                cores = getOption("mc.cores", 12), 
                file = "output/redmod2")
# summary(redmod)

# Intercept-only model
intmod <- brm(formula = DEATH_EVENT ~ 1, 
                data = heart,
                family = bernoulli(link = "logit"),
                chains = 4, 
                iter = 2000,
                warmup = 1000,
               control = list(max_treedepth = 20),
                cores = getOption("mc.cores", 12), 
                file = "output/intmod2")
# summary(intmod)

# Horseshoe model
if(file.exists("output/hs_mod.rds")){
	hs_mod <- read_rds("output/hs_mod.rds")
} else {
	D <- ncol(heart) - 1
	n <- nrow(heart)
	p0 <- 2 # prior guess for the number of relevant variables
	sigma <- 1 / sqrt(mean(heart$DEATH_EVENT) * (1-mean(heart$DEATH_EVENT))) # pseudo sigma
	tau0 <- p0/(D-p0) * sigma/sqrt(n)
	prior_coeff <- hs(df=1, global_df=1, global_scale = tau0) # tau âˆ¼ half-Cauchy(0, tau0^2)

	hs_mod <- stan_glm(DEATH_EVENT ~ ., 
                        data = heart, 
                        family = binomial(), 
                        prior = prior_coeff,
                        chains = 4, 
                        iter = 2000,
                        warmup = 1000,
                        control = list(max_treedepth = 20),
                        cores = getOption("mc.cores", 8), 
                        algorithm = "sampling")
	saveRDS(hs_mod, file = "output/hs_mod.rds")
}
# summary(hs_mod)
```

## In-sample checks
After the models were tuned and finalized, we performed in-sample checks. This included binned residual plots plotted against ejection fraction and serum creatinine (the two variables deemed most important by the reference paper the horseshoe prior model). A log transformation was completed on the serum creatinine variable in these plots for readibility. These binned residuals generally looked okay, though the no predictor model may have an issue of over predicting survival at low ejection fraction levels (see Appendix).  

Following our assessment of residuals, we performed posterior predictive checks using three summary statistics. These test quantities were created to evaluate any discrepancies between the model simulations and the true data in terms of predicted survival proportions under three scenarios: overall, among patients with a normal ejection fraction, and among patients with normal serum creatinine levels. A healthy range for ejection fraction is defined as greater than 40% in the paper by Chicco & Jurman while a healthy serum creatinine level is less than 1.2 mg/dL (Mayo Clinic). The test statistics are labeled as follows:  

- T1: Proportion of survival

- T2: Proportion of survival among patients with an ejection fraction $> 40\%$  

- T3: Proportion of survival among patients with serum creatinine < 1.2 mg/dL

## Out-of-sample checks
We then perform leave-one-out (LOO) cross-validation on the four models and use the resulting ELPD values, along with evaluation metrics suited for binary outcome data like Matthew's Correlation Coefficient (MCC), true positive rate, true-rate, accuracy, and ROC area under the curve. These additional metrics are taken from the reference paper and allow for comparison against Chicco & Jurman's machine learning models. We do not include a  


# Results

```{r, include = F}
# Posterior prediction for full model, intercept-only model, 
# and reduced model
ynewfull <- posterior_predict(fullmod)
ynewint <- posterior_predict(intmod)
ynewred <- posterior_predict(redmod)
```

```{r, include = F}
# Point estimates for all models
ytildefull <- apply(ynewfull, 2, mean)
ytildeint <- apply(ynewint, 2, mean)
ytildered <- apply(ynewred, 2, mean)
```

```{r, include = F}
# Residuals for all models
resfull <- heart$DEATH_EVENT - ytildefull
resint <- heart$DEATH_EVENT - ytildeint
resred <- heart$DEATH_EVENT - ytildered
```

```{r, include = F}
# Posterior prediction
ynew_si <- posterior_predict(hs_mod)
# Point estimates for all models
ytildehs <- apply(ynew_si, 2, mean)
# Residuals for all models
reshs <- heart$DEATH_EVENT - ytildehs
```


```{r, echo = F}
par(mfrow = c(2, 2))
full_res_ejection <- binnedplot(heart$ejection_fraction, resfull, xlab = "Ejection fraction", main = "Full model")
int_res_ejection <- binnedplot(heart$ejection_fraction, resint, xlab = "Ejection fraction", main = "Intercept-only model")
red_res_ejection <- binnedplot(heart$ejection_fraction, resred, xlab = "Ejection fraction", main = "Reduced model")
hs_res_ejection <- binnedplot(heart$ejection_fraction, reshs, xlab = "Ejection fraction", main = "Horseshoe prior model")
mtext(text = 'Model residuals plotted against ejection fraction', side = 3, line = 0, outer = TRUE)
```

```{r, echo = F}
par(mfrow = c(2, 2))
full_res_creatinine <- binnedplot(log(heart$serum_creatinine), resfull, xlab = "log(serum creatinine)", main = "Full model")
int_res_creatinine <- binnedplot(log(heart$serum_creatinine), resint, xlab = "log(serum creatinine)", main = "Intercept-only model")
red_res_creatinine <- binnedplot(log(heart$serum_creatinine), resred, xlab = "log(serum creatinine)", main = "Reduced model")
hs_res_creatinine <- binnedplot(log(heart$serum_creatinine), reshs, xlab = "log(serum creatinine)", main = "Horseshoe prior model")
mtext(text = 'Model residuals plotted against log serum creatinine', side = 3, line = 0, outer = TRUE)
```

All four models showed very accurate predictions for survival proportion without restriction on type of patient (T1). However, predictions for survival among patients with a normal ejection fraction (T2) was overestimated by every model except for the intercept-only model, which under predicted survival for this group. However, the horseshoe prior model had the best posterior predictive p-value of 0.766 by about 0.07. For T3, the proportion of survival for patients with normal serum creatinine levels, all models performed more similarly by underestimating survival proportion with all predictive p-values less than 0.25.  

Shown here is T2 for all of the models. For that test statistic, the full model has a predictive p-value of 0.836, the reduced model 0.940, the intercept-only model 0.094, and the horseshoe model 0.766. The horseshoe model performed the best in the case of patients with ejection fraction $> 40\%$.

```{r, echo = F}
true_t2 <- nrow(filter(heart, DEATH_EVENT == 0, ejection_fraction > 40)) / nrow(filter(heart, ejection_fraction > 40))

# Full model
ynew_ejecfull <- t(ynewfull) %>% 
  as_tibble() %>% 
  cbind(heart$ejection_fraction) %>% 
  filter(`heart$ejection_fraction` > 40)

hs_t2full <- sapply(1:(ncol(ynew_ejecfull) - 1), FUN = function(x) mean(ynew_ejecfull[, x] == 0))

t2_plotfull <- ggplot(data = as_tibble(hs_t2full), aes(value)) + 
  geom_histogram(aes(fill = "replicated")) + 
  geom_vline(aes(xintercept = true_t2, color = "observed"), lwd = 1.5) + 
  ggtitle("Full model") + 
  theme_bw(base_size = 12) + 
  scale_color_manual(name = "", values = c("observed" = "darkblue")) + 
  scale_fill_manual(name = "", values = c("replicated" = "lightblue"))
  
# Reduced model
ynew_ejecred <- t(ynewred) %>% 
  as_tibble() %>% 
  cbind(heart$ejection_fraction) %>% 
  filter(`heart$ejection_fraction` > 40)

hs_t2red <- sapply(1:(ncol(ynew_ejecred) - 1), FUN = function(x) mean(ynew_ejecred[, x] == 0))

t2_plotred <- ggplot(data = as_tibble(hs_t2red), aes(value)) + 
  geom_histogram(aes(fill = "replicated")) + 
  geom_vline(aes(xintercept = true_t2, color = "observed"), lwd = 1.5) + 
  ggtitle("Reduced model") + 
  theme_bw(base_size = 11) + 
  scale_color_manual(name = "", values = c("observed" = "darkblue")) + 
  scale_fill_manual(name = "", values = c("replicated" = "lightblue")) 


# Intercept-only model
ynew_ejecint <- t(ynewint) %>% 
  as_tibble() %>% 
  cbind(heart$ejection_fraction) %>% 
  filter(`heart$ejection_fraction` > 40)

hs_t2int <- sapply(1:(ncol(ynew_ejecint) - 1), FUN = function(x) mean(ynew_ejecint[, x] == 0))

t2_plotint <- ggplot(data = as_tibble(hs_t2int), aes(value)) + 
  geom_histogram(aes(fill = "replicated")) + 
  geom_vline(aes(xintercept = true_t2, color = "observed"), lwd = 1.5) + 
  ggtitle("Intercept-only model") + 
  theme_bw(base_size = 10) + 
  scale_color_manual(name = "", values = c("observed" = "darkblue")) + 
  scale_fill_manual(name = "", values = c("replicated" = "lightblue")) 

# T_2: prop(DEATH_EVENT == 0 | ejection_fraction > 40)
ynew_ejection <- t(ynew_si) %>%
	as_tibble() %>%
	cbind(heart$ejection_fraction) %>%
	filter(`heart$ejection_fraction` > 40)
hs_t2 <- sapply(1:(ncol(ynew_ejection)-1), FUN = function(x) mean(ynew_ejection[,x] == 0))
true_t2 <- nrow(filter(heart, DEATH_EVENT==0, ejection_fraction > 40)) / nrow(filter(heart, ejection_fraction > 40))
 
t2_ploths <- ggplot(data = as_tibble(hs_t2), aes(value)) + 
    geom_histogram(aes(fill = "replicated")) + 
    geom_vline(aes(xintercept = true_t2, color = "observed"), lwd = 1.5) + 
  ggtitle("Horseshoe prior model") + 
  theme_bw(base_size = 10) + 
  scale_color_manual(name = "", values = c("observed" = "darkblue"))+
  scale_fill_manual(name = "", values = c("replicated" = "lightblue"))
  
t2_plotfull + t2_plotred + t2_plotint + t2_ploths +
  plot_annotation(title = 'Survival proportion for normal ejection fraction (> 40%)') +
  plot_layout(ncol = 2, guides='collect') &
  theme(legend.position='bottom')
```

Afterward, out-of-sample model checks were performed. Leave-One-Out (LOO) cross-validation was performed on all of the models. Aside from 3 out of the 299 examples in the full model and 1 example in the horseshoe model which were defined as "okay" (< 0.7), all of the validations ended up with every value being "good" (< 0.5). 


```{r, include = F}
loo_full <- loo(fullmod, save_psis = TRUE)
loo_red <- loo(redmod, save_psis = TRUE)
loo_int <- loo(intmod, save_psis = TRUE)
loo_hs <- loo(hs_mod, save_psis = TRUE)
```

```{r, echo = F}
par(mfrow=c(2, 2))
plot(loo_full, diagnostic = c("k"), label_points = TRUE, main = "Full model")
plot(loo_red, diagnostic = c("k"), label_points = TRUE, main = "Reduced model")
plot(loo_int, diagnostic = c("k"), label_points = TRUE, main = "Intercept-only model")
plot(loo_hs, diagnostic = c("k"), label_points = TRUE, main = "Horseshoe prior model")

mtext(text = 'PSIS diagnostic plot', side = 3, line = 0, outer = TRUE)
```

# Conclusion and discussion

# Appendix

## Source code

## Supplemental figures